K8S architecture 
=================
Kubernetes (k8s) is an open-source container orchestration platform that automates the deployment, scaling, and management of 
containerized applications. Its architecture consists of several components distributed across master and worker nodes.
Let's break down the components and their functionalities:

User Interface (UI):-
Kubernetes provides a web-based user interface for interacting with the cluster and allows users to manage applications, 
troubleshoot issues,and monitor cluster health.

Command-Line Interface (CLI):-
Kubernetes CLI, commonly referred to as kubectl, is a command-line tool used to interact with Kubernetes clusters. It allows users to 
perform various operations such as deploying applications, managing resources, inspecting cluster status, and executing
administrative tasks.

Master Node Components:
--------------------------
API Server:
Acts as the front-end for the Kubernetes control plane, Exposes the Kubernetes API
CLI TOOLS like kubectl, users & even master components like sheduler, contoller manager, etcd and worker node components like kubelet
everything is talks with API-SERVER

Scheduler:
Responsible for distributing containers(pods) across the worker nodes in the cluster.
Monitors the availability of resources on each node and selects the most suitable node for placing pods based on resource requirements,
affinity/anti-affinity rules, and other constraints.
Enables efficient resource utilization and workload balancing across the cluster.

Controller Manager:
"The Controller Manager in Kubernetes acts like a supervisor, making sure all parts of the cluster are doing their jobs correctly. 
It keeps an eye on things and fixes any issues to maintain the cluster's desired setup. For example, it ensures the right number of 
apps are running and that services are accessible."
Examples of controllers include ReplicaSet, StatefulSet, Deployment, Service, PersistentVolume, Namespace, and Node controllers.

etcd:
consistent(keeping everything in sync) and highly available key value store and used as K8S banking store for all cluster data
it stores all the master and workernode information.

Kubelet:
Manages the containers running on a node, ensuring they are healthy and running as desired.
Communicates with the API server to receive instructions and report the status of containers.

Kube-proxy:
Manages network connectivity to services running in the cluster.
Implements network rules to enable communication between pods and external clients, ensuring efficient routing and load balancing.

================================================================

 Controller Manager:- in Kubernetes is responsible for maintaining the desired state of the cluster by managing various controllers. 
These controllers continuously monitor the state of the cluster and reconcile any differences between the current state 
and the desired state declared by users or applications. Here's a more detailed overview of the Controller Manager's role
and the controllers it manages:

Replication Controller:
Ensures that the specified number of pod replicas are running at all times.
Monitors pod health and automatically creates or deletes replicas to match the desired count.

Namespace Controller:
Manages namespaces within the cluster, ensuring their creation, deletion, and updates are handled correctly.
Enforces resource quotas and policies specific to each namespace.

Node Controller:
Monitors the state of nodes in the cluster.
Detects when nodes become unreachable or are removed from the cluster and takes appropriate action, such as rescheduling pods.

Service Account & Token Controller:
Manages service accounts and tokens used for authentication within the cluster.
Ensures that necessary service accounts and tokens are created and available to pods.

Endpoints Controller:
Populates the Endpoints resource to reflect the current set of endpoints (i.e., IP addresses and ports) for services.
Updates endpoints based on changes to the service's selector or its backing pods.

Service Controller:
Monitors the creation, deletion, and updates of service objects.
Ensures that services are properly configured and accessible within the cluster.

Persistent Volume Controller:
Manages the lifecycle of PersistentVolume objects.
Ensures that PersistentVolumes are provisioned, bound to PersistentVolumeClaims, and dynamically provisioned or deleted as needed.

StatefulSet Controller:
Manages the deployment and scaling of stateful applications, ensuring ordered pod creation, deletion, and updates.
Maintains stable network identities and persistent storage for stateful pods.

DaemonSet Controller:
Ensures that a specified pod runs on all or a subset of nodes in the cluster.
Automatically creates or deletes pods to match node labels and selectors.

Job & CronJob Controllers:
Manages batch workloads, such as one-off jobs or recurring tasks.
Creates and manages pods to execute Jobs and CronJobs according to specified schedules.
These controllers work together under the supervision of the Controller Manager to ensure that the cluster remains in the desired state,
providing resilience, scalability, and efficient resource utilization for containerized workloads.


How we can change the hostname
==============================
goto root user-----> sudo -i
then after we can execute the command-----> hostname <name-of-the-host> && bash



https://raw.githubusercontent.com/vsaini44/KubernetesRepo/master/Docker-kuber1.21
===================================================================================
# by using the above link we can get the script place it in a file then after execute that file
# for example the file name is ---->a 
# so here we can execute that file by using the command------> bash a  (or) ./a.sh
# after successful execution we can get the {-- 1 token --} in the master machine we can copy that token and execute over
the Node machines

# script we have to execute
============================
#!/bin/bash 
sudo apt update

sudo apt install apt-transport-https ca-certificates curl software-properties-common  -y
curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -
sudo add-apt-repository "deb [arch=amd64] https://download.docker.com/linux/ubuntu bionic stable"
sudo apt update
apt-cache policy docker-ce -y
sudo apt install docker-ce -y
wget -q -O - https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -
echo deb http://apt.kubernetes.io/ kubernetes-xenial main | sudo tee /etc/apt/sources.list.d/kubernetes.list
apt update
apt install kubelet=1.21.1-00 kubeadm=1.21.1-00 kubectl=1.21.1-00 -y
sysctl net.bridge.bridge-nf-call-iptables=1

#on master
#kubeadm init --pod-network-cidr=10.244.0.0/16 >> cluster_initialized.txt
kubeadm init --pod-network-cidr=192.168.0.0/16 >> cluster_initialized.txt
mkdir /root/.kube
cp /etc/kubernetes/admin.conf /root/.kube/config
kubectl apply -f https://docs.projectcalico.org/manifests/calico.yaml
#kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
systemctl restart kubelet.service
kubeadm token create --print-join-command



# Script we have to execute in the Node machines
=================================================

#!/bin/bash 
sudo apt update

sudo apt install apt-transport-https ca-certificates curl software-properties-common  -y
curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -
sudo add-apt-repository "deb [arch=amd64] https://download.docker.com/linux/ubuntu bionic stable"
sudo apt update
apt-cache policy docker-ce -y
sudo apt install docker-ce -y
wget -q -O - https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -
echo deb http://apt.kubernetes.io/ kubernetes-xenial main | sudo tee /etc/apt/sources.list.d/kubernetes.list
apt update
apt install kubelet=1.21.1-00 kubeadm=1.21.1-00 kubectl=1.21.1-00 -y
sysctl net.bridge.bridge-nf-call-iptables=1

=================================================================================
or else we can execute the commands one by one

to get libraries
=================
kubectl completion bash >> /tmp/abc
source  /temp/abc
kubectl <press tab two-times>

the bellow commands will say which paramiter we have to use in perticular yml file
====================================================================================
kubectl explain pod
kubectl explain service
kubectl explain repicaset
kubectl explain replicationcontroller
kubectl  explain deployment

Command to show the labels
==========================
kubectl get pods --show-labels

 Commandto set a label to the perticular pod
========================================
kubectl label pod <pod-name> env=<label-name>
EX:- kubectl label pod pod1 env=dev1

if you want to override the env by using the bellow command
==============================================================
kubectl label pod pod1 env=prod --override


kubectl create -f pod.yaml
kubectl get pods
vim pod.yaml
 
kubectl get pods
kubectl get pods  --show-label
kubectl get pods  --show-labels
kubectl label pod pod1 env=dev
kubectl label pod pod2 env=prod
kubectl get pods  --show-labels
vim pod.yaml
kubectl create -f pod.yaml
kubectl get pods  --show-labels
kubectl get pods -l env=prod

kubectl run image
kubectl run image --help
cat pod.yaml
kubectl get pods --show-labels
kubectl label pod pod1 dc=mumbai
kubectl get pods --show-labels
kubectl label pod pod1 env=prod
kubectl label pod pod1 env=prod --overwrite
kubectl get pods --show-labels
kubectl describe pod pod1 | less
kubectl annotate pod pod1 description="test pod for testing"
kubectl describe pod pod1 | less
kubectl get ns
kubectl create ns test-project1
kubectl get ns | grep test-project

How do create the NameSpace 
============================
if you execute the command---> kubectl get ns
it will redirected to the default namespace
if we want to see in which namespace pointing by default by using the command
--> kubectl config view
           (or)
--> kubectl config view --minify           

so that by default the pods are created inside default namespace. here if we don't want to create the pods inside the default
namespace we can create our own namespace by using the bellow command
--> kubectl create ns/namespace <namespace-Name>
ex:-->  kubectl create ns flipkartapp
after that we if can see is any pods are created in the flipkartapp namespace by using the command
-->kubectl get all -n filpkartapp
so if want execute the flipkartapp is the default namespace you can execute bellow command
Syntax:--> kubectl config set-context --current --namespace=<give name of namespace>
Ex:--> kubectl config set-context --current --namespace=flipkartapp

So now if we want to see in which namespace pointing by default by using the command
--> kubectl config view
-->kubectl get all

so again if you want to set namespace as default, execute the bellow command
Ex:--> kubectl config set-context --current --namespace=default
   --> kubectl get all
   

Example:- Pod manifast YAML file and the file name is javawebapppod.yaml
=========================================================================
apiVersion: v1
kind: Pod
metadata:
  name: javawebapppod
  labels:
    app: javawebapp
  namespace: flipkartapp
spec:
  containers:
  - name: javawebappcontainer
    image: dockerhandson/java-web-app
    ports:
    - containerPort: 8080
   
now if we check if any pods are running in the flipcartapp namespace or not by using the bellow command
--> kubectl get pod -n flipkartapp
then after we can execute the bellow command
--> kubectl apply -f javawebapppod.yaml
now the pods are created at flipkartapp namespace 

so that if we want to see in which namespace which pods are running at a time by using the command
--> kubectl get all --all-namespaces

what is the diff between Docker service and K8S service?
========================================================
here using docker service in docker-swarm we can deploye the containers/multiple (or) single replicas,and docker will manage those containers.
and comming to the K8S service, K8S service is responsible for making our pods accessable in side the network(cluster) or Exposing them to the internet
:-) we can use the K8S service to access the application's which are running inside the pod with in the cluster (or)outside the cluster

note: here Service can identify the pods by using labels and selector

=> here service can be classified into three types
   --> ClusterIp:- if we create the service using ClusterIp we can access the pod with in the cluster
   --> NodePort
   --> LoadBalancer
   what is the nodeport range in the K8S?
   =======================================
   nodeport range is 30000 to 32767
   
   Example for service YAML file name is javawebappservice.yaml
   ============================================================
   apiVersion: v1
   kind: Service
   metadata:
     name: javawebappservice
     namespace: flipkartapp
   spec:
     type: NodePort
     selector:
       app: javawebapp
     ports:
     - port: 80
       targetPort: 8080
       nodeport: 30033
       
 Note: if we won't give nodeport also K8S will randamly genarate the nodeport
 
 --> kubectl apply -f javawebappservice.yml 
 

#ReplicationController Manifest Syntax
=======================================
apiVersion: v1
kind: ReplicationController
matadata:
   name: <RC-name>
   namespace: <namespace-name>
spec:
  replicas: <no-of-pod-replicas>
  selector:
    <key>: <value>   # POD label key & value should be defind here
  template:          # Pod template(define Pod information)
    metadata:
      name: <POD name>
      labels:
        <key>: <value>   # Labels of pod
    spec:
      containers:
      - name: <name-of-the-container>
        image: <image-name>
        ports:
        - containerPort: <containerPort>
        
Example for RC and name of the RC is javawebapp_rc.yaml
========================================================
apiVersion: v1
kind: ReplicationController
matadata:
   name: javawebapprc
   namespace: flipkartapp
spec:
  replicas: 2
  selector:
    app: javawebapp   # POD label key & value should be defind here
  template:          # Pod template(define Pod information)
    metadata:
      name: javawebapppod
      labels:
        app: javawebapp   # Labels of pod
    spec:
      containers:
      - name: javawebappcontainer
        image: dockerhandson/java-web-app
        ports:
        - containerPort: 8080


 what is the diff b/w ReplicaSet and ReplicationController?
 ============================================================
  ReplicationController it will manage the pod lifecycle, we can scaleUp & scaleDown the pods and ReplicaSet also does same thing
  the only difference in selector support ReplicationController supports only Equality based selector and Replicaset supports Equality based
  as well as set based selectors. 
  Example Syntaxes:
  =================
  # RC Equality based selector
  spec:
    selector:
      <key>: <value>
      
   #RS
   spec:
     selector:                  # Equlity Based selector
       matchLabels:
         <key>: <value>
         
  #RS
  spec:
    selector:
      matchExpression:           # Set Based selector
      - key: <labelkey>
        operator: in
        values:
        - <valueOne>
           <valueTwo>
           
 Example for RS and name of the RS is javawebapp_rs.yaml
========================================================
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: javawebapprs
spec:
  replicas: 2
  selector:
    matchLabels:
      app: javawebapp
    template:
      metadata:
        name: javawebapppod
        labels:
          app: javawebapp
      spec:
        containers:
        - name: javawebappcontainer
          image: dockerhandson/java-web-app
          ports:
          - containerPort: 8080
          


# Daemonset
===========
By using Demonset we can't scaleup & scaledown the DemonSet.
A DaemonSet make sure that each machine have a copy of the pod,but here we can't scaleup & scaledown
here the manifest is exectly same as replicaset but here we have to change the kind as DaemonSet and here
replicas are not allowed

Example for DS and name of the DS is mavenwebapp_ds.yaml
========================================================
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: mavenwebappDc
spec:
  selector:
    matchLabels:
      app: mavenwebapp
  template:
    metadata:
      name: mavenwebapppod
      labels:
        app: mavenwebapp
    spec:
      containers:
      - name: mavenwebappcontainer
        image: dockerhandson/maven-web-application
        ports:
        - containerPort: 8080
        

run ---> kubectl apply -f mavenwebapp_ds.yaml

Note: if we delete the pod also it will create another pod here because A DaemonSet make sure that each machine
have a copy of the pod

what kind of images/apllications we can deploye as DaemonSet?
==============================================================
internal applications like loging, Monitoring applications
--> Loging applications like splunk,
--> Monitoring applications which will monitoring that servers
 
# Deployement
==============
here Deployement is create the ReplicaSet And ReplicaSet will create the pod, in K8S Deployement 
is the recommended way to deploye the Pod (or) ReplicaSet. because of the advanced features it comes with
Below are some of the key benefits.
>>RollBack
>>RollOut
>>we can maintain the Zero DownTime Deployment 

kubectl scale deployment [DEPLOYMENT_NAME] --replicas [NUMBER_OF_REPLICAS]
kubectl rollout status deployment nginx-deployment
kubectl get deployment
kubectl set image deployment nginx-deployment nginx-container=nginx:latest --record 
kubectl get replicaset
kubectl rollout history deployment nginx-deployment
kubectl rollout history deployment nginx-deployment --revision=1
kubectl rollout undo deployment nginx-deployment --to-revision=1

Note:-
=======
Deployment have two types of strategies they are 
--> recreate  
--> rollingUpdate

# Deployement Manifest Syntax
==============================
apiVersion: apps/v1
kind: Deployment
metadata:
  name: <deployment-Name>
  namespace: <namespace-Name>
spec:
  replicas: <noofReplicas>
  selector:
    matchLabels:
      <labelKey>: <value>
  strategy:                             # here the strategy's are come into the pitcher,When new updations happens 
    type: <deplymentStrategy>
  template:
    metadata:
      name: <PodName>
      labels:
        <key>: <values>
    spec:
      containers:
      - name: <NameOfTheContainer>
        image: <imageName>
        ports:
        - containerPort: <containerPort>
        
# Eample Deployment maniFest for the strategy Recreate file name is deployement_recreate.yaml
=============================================================================================
apiVersion: apps/v1
kind: Deployement
metadata:
  name: javawebappdeployement
spec:
  replicas: 2
  strategy:
    type: Recreate
  selector:
    matchLabels:
      app: javawebapppod
  template:
    metadata:
      name: javawebapppod
      labels:
        app: javawebapp
    spec:
      containers:
      - name: javawebappcontainer
        image: dockerhandson/java-web-app:1
        ports:
        - containerPort: 8080
        
run-->kubectl apply -f deployement_recreate.yml --record  
            {NOTE:- here if we won't use --record, we did't get the change cause, like for what resion the deployment updated}
      kubectl get deployement
      kubectl get rs
      kubectl get pods
      
      to know the status of deployement
      ==================================
      kubectl rollout status deployment javawebappdeployment
      
      to know the history of deployement
      ===================================
      kubectl rollout history deployment javawebappdeployment
      
      to know the more details about history
      ======================================
      kubectl rollout history deployment javawebappdeployment --revision=1
      
Note:
     for example if we want to upgrade the our application from version1 to version2 by using strategy Recreate first 
     it will remove the containers at a time and then it will recreate the containers so that here we can face downtime
     
    here we can upgrade the application in two ways
    ---> by using the command 
     Syntax:-  kubectl set image deployment/<Deployment-Name> <Container-Name>=<Container-Image> --record=true
            kubectl set image deployment javawebappdeployment javawebappcontainer=dockerhandson/java-web-app:2 --record   
            
           
    ---> update the manifest file deployement_recreate.yaml
                here you can update the version of the image and run the command 
                ==> kubectl apply -f deployement_recreate.yaml --record
     
      to know the status of deployement
      ==================================
      kubectl rollout status deployment javawebappdeployment
      
      to know the history of deployement
      ===================================
      kubectl rollout history deployment javawebappdeployment
      
      to know the more details about history
      ======================================
      kubectl rollout history deployment javawebappdeployment --revision=2
   
   
   So if we want to rollback to the previous revision
         ==> kubectl rollout history deployment javawebappdeployment --revision=1
         after executing the above command again the conatainers will created with version1
          image: dockerhandson/java-web-app:1
          
 for example if the latest version is not working as excepted we can rollback to the previous revision by using the command
Syntax:-  kubectl rollout undo deployement <deploymentname> --to-revision:1
Example:- kubectl rollout undo deployement javawebappdeployment --to-revision:1
Note:
     if we don't use --to-revision also it will rollback one revision, if we menction the opction --to-revision:<VersionNumber> 
     we can rollback any other versions also, here also we can face downtime because here we are using recreate strategy.
     
     
# Rolling Restarts
===================
>> Rolling restarts will kill the existing pods and recreate new pods in a rolling fashion.
kubectl rollout restart deployment/<Deployment-Name>
kubectl rollout restart deployment/my-first-deployment

# Get list of Pods
kubectl get po
     
#Rolling Deployement
====================
The rolling deployment is the standard default deployment to Kubernetes. It works by slowly, one by one, 
replacing pods of the previous version of your application with pods of the new version without any 
cluster downtime.
A rolling update waits for new pods to become ready before it starts scaling down the old ones. If there is 
a problem, the rolling update or deployment can be aborted without bringing the whole cluster down. In 
the YAML definition file for this type of deployment, a new image replaces the old image.

# Eample Deployment maniFest for the strategy Recreate file name is deployement_rollingupdate.yaml
==================================================================================================
apiVersion: apps/v1
kind: Deployement
metadata:
  name: javawebappdeployement
spec:
  replicas: 2
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 1
  minReadySeconds: 30
  selector:
    matchLabels:
      app: javawebapppod
  template:
    metadata:
      name: javawebapppod
      labels:
        app: javawebapp
    spec:
      containers:
      - name: javawebappcontainer
        image: dockerhandson/java-web-app:1
        ports:
        - containerPort: 8080

maxSurge: 1
-----------
This defines the maximum number of pods that can be created above the desired number of replicas during an update. 
Setting it to 1 means that the deployment will add one new pod at a time while updating.

maxUnavailable: 1
-----------------
This indicates the maximum number of pods that can be unavailable during an update. Setting it to 1 allows one pod 
to be down during an update, ensuring that at least one pod remains running.

minReadySeconds: 30
----------------------
This sets the minimum time a pod must be ready (i.e., operational) before it's considered available. 
This parameter helps ensure that the new pod is fully up and running, handling traffic, and ready for service
before the old pod is terminated.
        
Run ---> kubectl apply -f deployement_rollingupdate.yaml


what is Blue/Green deployement (or) Red/Block Deployement?
===========================================================
In a blue/green deployment strategy (sometimes referred to as red/black) the old version of the application (green) and 
the new version (blue) get deployed at the same time. When both of these are deployed, users only have access to the 
green; whereas, the blue is available to your QA team for test automation on a separate service or via direct portforwarding.
After the new version has been tested and is signed off for release, the service is switched to the blue version with the 
old green version being scaled down


Volumes
=======
Note:-- if you configure the storage class, you will create the PVC(persistent volume claim) directly, claim will create the volume
        with the help of storage class

Note:-- while creating the volume we can menction the access mode. And it will not support all the access modes
        hostPath---> supports  ReadWriteOnce
        EBS-------->  supports  ReadWriteOnce
        if we want multiple nodes to mount a single volume -------->supports ReadWriteMany

AccessModes
===========
ReadWriteOnce--> volume can be mounted as read-write by single node
ReadWriteMany--> volume can be mounted as read-write by many node-----> supports nfs
ReadOnlyMany--> volume can be mounted as read-only by many node

Note:- here PVC is directly associated with pv. So while creating the pvc we can give a clime polocies

Cliam polocies
===============
Retain---> when pvc is deleted. pv will not be deleted 
Delete----> when we delete  pvc. it will delete pv and also associated external storage such as
            AWS EBS,GPED, AzureDisk.
Recycle--->when we delete pvc it will not delete pv. but it will remove data from the storage.


Here i have 2 persistent volumes when you are creating the PVC, what opctions/factors it will use to associate with PV? 
======================================================================================================================
it will match with  storageClassName
                    accessmodes
                    requests
For example i am tryning to create a pvc with AccessMode ReadWriteOnce, suppose if i have a two volumes with ReadWriteOnce,
here one is already claimed by some one else and one is avilable, here it will consider the volume which is not climed by any one 



 AutoScaling of Pods Based on the application lode
 ==================================================
 HPA and METRIC SERVER

HPA
====
The Horizontal Pod Auto scaler automatically scales the number of pods in a replication controller,
deployment, replica set based on observed CPU utilization or memory utilization. The Horizontal Pod 
Auto scaler is implemented as a Kubernetes API resource and a controller. The resource determines the
behavior of the controller. The controller periodically adjusts the number of replicas in a replication 
controller or deployment to match the observed average CPU/memory utilization to the target specified by user.
HPA will interact with Metric Server to identify CPU/Memory Utilization of POD.


Kubernetes Metric Server
Metric Server is an application that collects metrics from objects such as pods, nodes according to the state of CPU, RAM and keeps them in time.

Metric-Server can be installed in the system as an addon. You can take and install it directly from the repo.

===Setup Metrics Server===

Git Clone below repo in Kubernetes Master or in kubectl client Machine.

$ git clone https://github.com/MithunTechnologiesDevOps/metrics-server.git

$ cd metrics-server

$ kubectl apply -f deploy/1.8+/


When the Metric server is installed directly, it is installed under the kube-system namespace


Usage
# Display node metrics
$ kubectl top nodes

# Display pod metrics
$ kubectl top pods


Refer below link for more details :

 https://github.com/kubernetes-sigs/metrics-server 


We completed the first part in the article under the first phase, now let’s create a simple deployment object and the HPA structure
attached to it and observe its behaviour.

Get below manifest files from 

https://github.com/MithunTechnologiesDevOps/Kubernates-Manifests/blob/master/horizontal-pod-autoscaler.yml 

# Deployment
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: hpadeployment
  labels:
    name: hpadeployment
spec:
  replicas: 2
  selector:
    matchLabels:
      name: hpapod
  template:
    metadata:
      labels:
        name: hpapod
    spec:
      containers:
        - name: hpacontainer
          image: k8s.gcr.io/hpa-example
          ports:
          - name: http
            containerPort: 80
          resources:
            requests:
              cpu: "100m"
              memory: "64Mi"
            limits: 
              cpu: "100m"
              memory: "256Mi"
# HPA For above deployment(hpadeployment)
---
apiVersion: autoscaling/v2beta1
kind: HorizontalPodAutoscaler
metadata:
  name: hpadeploymentautoscaler
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: hpadeployment
  minReplicas: 2
  maxReplicas: 5
  metrics:
    - resource:
        name: cpu
        targetAverageUtilization: 50
      type: Resource

# Service
---
apiVersion: v1
kind: Service
metadata:
  name: hpaclusterservice
  labels:
    name: hpaservice
spec:
  ports:
    - port: 80
      targetPort: 80
  selector:
    name: hpapod
  type: ClusterIP


Deploy all above three kubernetes objects.

# ==== Execute below commands to increase load====

Now, we will see how the auto scaler reacts to increased load. We will start a container, and send an infinite loop of 
queries to the php-apache service .
# Create A Temp Pod in interactive mod to access app using service name

  $ kubectl run -i --tty load-generator --rm  --image=busybox /bin/sh

# Execute below command in Temp Pod
 
  $ while true; do wget -q -O- http://hpaclusterservice; done

Open kubectl terminal in another tab and watch kubectl get pods or kubect get hpa to see how the auto scaler reacts to increased load. 

  $ watch kubectl get hpa


Requests and Limits
---------------------

Requests and limits are the mechanisms Kubernetes uses to control resources such as CPU and memory. 

Requests are what the container is guaranteed to get. If a container requests a resource, Kubernetes will only schedule it on a node that can give it that resource.
 
Limits, on the other hand, make sure a container never goes above a certain value. The container is only allowed to go up to the limit, and then it is restricted.
It is important to remember that the limit can never be lower than the request. If you try this, Kubernetes will throw an error and won’t let you run the container.
Requests and limits are on a per-container basis. While Pods usually contain a single container, it’s common to see Pods with multiple containers as well.
Each container in the Pod gets its own individual limit and request, but because Pods are always scheduled as a group, you need to add the limits and requests
for each container together to get an aggregate value for the Pod.

Resource Configuration Values
-------------------------------
Memory is measured in bytes and expressed as an integer or using a fixed point integer.
For example; memory: 1 is 1 byte, memory: 1Mi is 1 mebibyte / megabyte, memory: 1Gi is 1 gibibyte / gigabyte. Memory is not a compressible resource, and there is no
throttling. Kubernetes evicts a pod from a node If a node cannot provide sufficient memory.

Note: One thing to keep in mind about CPU requests is that if you put in a value larger than the core count of your biggest node, your pod will never be scheduled.
      Let’s say you have a pod that needs four cores, but your Kubernetes cluster is comprised of dual core VMs—your pod will never be scheduled.


CPU is measure in millicpus, which is 1/1000th of a CPU core and expressed with integers. For example; cpu: "1" is 1 CPU, cpu: 2000m is 2 CPUs, cpu: "0.75" is 0.75 of a cpu and equivalent to cpu: 750m. CPU is a compressible resource and can be throttled by Kubernetes during contention. CPU contention does not cause pod eviction; instead, the app is slowed down.

Note: If you put in a memory request that is larger than the amount of memory on your nodes, the pod will never be scheduled.

How do we trableshoot crash loopbackup in K8s?
Difference between the replicaset and deployment?
          
  
