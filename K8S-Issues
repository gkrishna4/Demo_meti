What is CrashLoopBackOff?
==========================
your pods are starts, crashes, and Kubernetes keeps trying to restart it, but it keeps failing.

"The CrashLoopBackOff error in Kubernetes indicates that a container is repeatedly crashing after starting. I begin by describing the pod and checking
the logs using kubectl logs --previous to identify the root cause. Common issues include misconfigured environment variables, incorrect startup commands,
or application bugs. I also check for OOMKilled events, which suggest resource limits need adjustment. If probes are misconfigured, I disable them temporarily
to isolate the issue. Once identified, I fix the configuration or code and redeploy the pod. This structured approach helps ensure a quick and effective resolution."

When Do We Get This Error:
-------------------------
	The application inside the container crashes immediately after starting.
	There’s a misconfiguration (e.g., missing environment variables, wrong command).
	The container depends on a service that isn’t ready yet.
	There’s a bug in the application code.
	Resource limits (CPU/memory) are too low, causing OOM (Out of Memory) kills.

Step-by-Step Troubleshooting
-----------------------------
Step 1: Describe the Pod
	kubectl describe pod <pod-name>
	Look at the Events section for clues.
	Check for messages like Back-off restarting failed container.

Step 2: Check Container Logs
	kubectl logs <pod-name> --previous

	Use --previous to see logs from the last crashed container.
	Look for stack traces, error messages, or missing configs.


Step 3: Check the Pod Spec
	Look at the YAMLfile by using the commad: kubectl get pod <pod-name> -o yaml
	Verify:
		Correct command and args
		Required env variables
		Mounted volumes and paths

Step 4: Check Resource Limits
	If the pod is being killed due to memory: kubectl describe pod <pod-name> -n <namespace>

	Look for: OOMKilled
	Solution: Increase memory limits in the deployment YAML.

apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-app
spec:
  replicas: 1
  selector:
    matchLabels:
      app: my-app
  template:
    metadata:
      labels:
        app: my-app
    spec:
      containers:
      - name: my-container
        image: my-image:latest
        resources:
          requests:
            memory: "256Mi"
            cpu: "250m"
          limits:
            memory: "512Mi"
            cpu: "500m"

requests: Minimum resources guaranteed.
limits: Maximum resources the container can use.
If your app needs more memory, increase the limits.memory value (e.g., from 512Mi to 1Gi).

Apply the updated YAML: kubectl apply -f deployment.yaml
Monitor the pod: 
		kubectl get pods
		kubectl describe pod <pod-name>

Step 5: Use Liveness/Readiness Probes Carefully
	Misconfigured probes can cause restarts.
	Temporarily disable them to test
	EX:-
	----
		livenessProbe:
 			 initialDelaySeconds: 30
  	 		 periodSeconds: 10

Step 6: Debug with an Interactive Pod
	If needed, run a debug container: kubectl run -it debug --image=busybox --restart=Never -- sh
	Use this to inspect volumes, configs, or network.


===========================================================================================================================================================
 ImagePullBackOff Failures
===========================
It means that the pod is unable to pull the container image from the specified container registry.

"The ImagePullBackOff error in Kubernetes indicates that the kubelet on a node is unable to pull the container image from the registry. This can
happen due to reasons like incorrect image name, missing tags, private registry access issues, or network problems. To resolve it, I first describe
the pod using kubectl describe pod to check the exact error. If it's an authentication issue, I create a Kubernetes secret and attach it using
imagePullSecrets. If it's a typo or tag issue, I correct the image reference in the deployment YAML. I also verify network connectivity and try pulling
the image manually on the node. Once fixed, I redeploy the pod. This structured approach helps quickly identify and resolve the root cause."

First we can Describe the Pod:
-----------------------------------	
	kubectl describe pod <pod-name>

And Check for messages like:
	|- Failed to pull image
	|- unauthorized
	|- not found


Incorrect Image Name or Tag
-----------------------------
Check: Ensure the image name and tag are correct in your pod/deployment YAML.
Fix: Correct the image name or tag if there's a typo.

Private Registry Authentication Failure
--------------------------------------------
Check: If the image is hosted in a private registry (e.g., Docker Hub, AWS ECR, GCR), Kubernetes needs credentials.
Fix: Create a Kubernetes secret using:
		
kubectl create secret docker-registry myregistrykey \
  --docker-server=<your-registry-server> \
  --docker-username=<your-name> \
  --docker-password=<your-pword> \
  --docker-email=<your-email>
Then reference it in your pod spec under imagePullSecrets

Network Issues
-----------------
Check: Ensure the node can reach the registry (DNS resolution, firewall, proxy settings).
Fix: Resolve any network connectivity issues.

Rate Limits (e.g., Docker Hub)
----------------------------------
Check: Docker Hub enforces rate limits on anonymous and free-tier users.
Fix: Use authenticated pulls or mirror the image to another registry.

Image Does Not Exist
--------------------------
Check: Try pulling the image manually on a node:

docker pull <image-name>
Fix: Use a valid image that exists in the registry

===========================================================================================================================================================
cribe the Node: kubectl describe node <node-name>
	Check for:
		|- Disk pressure:
			Look for:
			  Conditions:
  				DiskPressure: True

			How to Fix:SSH into the node and execute the command: df -h
				   Clean up unused Docker images and containers : docker system prune -a
			           Clear logs or temporary files: sudo journalctl --vacuum-time=2d


		|- Memory pressure
		   	Node is low on available memory.
			Look for:
			    Conditions:
  				MemoryPressure: True
		
		    How to Fix: Identify memory-hungry pods: kubectl top pod --all-namespaces
				Evict or reschedule heavy pods to other nodes.
				Increase node memory (if using cloud provider).
				Adjust pod resource requests/limits to prevent overcommitment.
		
	       |- Network unavailability
		  	Node cannot communicate with the control plane or other nodes.
		  	How to Check: Ping the API server from the node : ping <api-server-ip>
		  	Check kubelet logs: journalctl -u kubelet
			
		 	 How to Fix:  Restart network services: sudo systemctl restart network
				      Check firewall rules or cloud security groups.
				      Ensure DNS is working: nslookup kubernetes.default

	      |-Kubelet errors
			 Kubelet is misconfigured or has crashed.

			How to Check: journalctl -u kubelet
			How to Fix: Restart kubelet : sudo systemctl restart kubelet
				    Check kubelet config file (/var/lib/kubelet/config.yaml) for errors.
				    Ensure correct certificates and API server address.
	Note:
		"When a node shows conditions like disk or memory pressure, or kubelet/network issues, I start by describing the node to identify the exact
                 problem. For disk pressure, I clean up unused Docker images and logs. For memory pressure, I check pod usage and reschedule if needed.
                 If there's network unavailability, I verify connectivity and DNS. For kubelet errors, I inspect logs and restart the service. This helps 
		 restore node health and ensure pods are scheduled properly."
	

Step 3: Check Node Logs: SSH into the node and check logs by using the command: journalctl -u kubelet
	Look for errors related to kubelet, container runtime, or system resources.

Step 4: Check Cloud Provider Console (if applicable)
	Ensure the VM/instance is running.
	Check for:
		Auto-scaling events
		Maintenance or preemption
		Network/firewall issues
Step 5: Drain and Reboot the Node (if recoverable)
	kubectl drain <node-name> --ignore-daemonsets --delete-emptydir-data
	Then reboot the node: sudo reboot

Step 6: Replace or Rejoin the Node: 
	If the node is permanently down, remove it by using the command: kubectl delete node <node-name>
	Add a new node or let the auto-scaler replace it.

Step 7: Monitor Cluster Health: kubectl get pods -A -o wide
	Ensure pods are rescheduled on healthy nodes.

===========================================================================================================================================================

Network connectivity issues between pods 
=========================================
In Kubernetes, every pod is assigned an IP address, and by default, all pods can communicate with each other across nodes unless restricted by network policies.

Network connectivity issues between pods can occur due to several reasons. Some common scenarios include:
     =>	Misconfigured or overly restrictive NetworkPolicies
	    => Problems with the CNI plugin (like Calico, Flannel, etc.)
	    => DNS resolution failures
	    => Firewall rules or cloud security groups blocking traffic
	    => IP conflicts or routing issues between nodes”.

When I face such issues, I follow a systematic approach:
--------------------------------------------------------
Ping or curl between pods using kubectl exec to confirm the issue.
		=> If ping fails, it could be a network policy, CNI, or routing issue.
		  	          Get the Pod Names:-   kubectl get pods -n <namespace>
 		 	          Ping Another Pod from Inside a Pod:-   kubectl exec -n <namespace> pod-a -- ping <pod-b-ip>

	      => If curl fails, it could be a service issue (e.g., app not listening on the port).
			           If the pod is running a web service (like on port 80 or 8080), you can test with curl:
                                   kubectl exec -n <namespace> pod-a -- curl http://<pod-b-ip>:<port>

Check for NetworkPolicies using kubectl get networkpolicy and review their rules.
	=>	List All NetworkPolicies:-   kubectl get networkpolicy -n <namespace>   note:- This will show you all the NetworkPolicies applied in that namespace.
	=>	Describe a Specific NetworkPolicy
			kubectl describe networkpolicy <policy-name> -n <namespace>
		     	This command gives you detailed information about:
				       >>   What pods the policy applies to
				       >>   What ingress (incoming) or egress (outgoing) traffic is allowed
			               >>   Which ports and IP blocks are permitted
			
		
	Inspect the CNI plugin logs in the kube-system namespace.
			 Identify the CNI Plugin Pods
				     kubectl get pods -n kube-system    ----> this command to list all pods in the kube-system namespace:  
				     Look for pods related to your CNI plugin. Examples:
					               >>  Calico: calico-node, calico-kube-controllers
					               >>  Flannel: kube-flannel-ds
					               >>  Cilium: cilium, cilium-operator
					               >>  Weave: weave-net
					View Logs for a CNI Pod
					               >> Once you identify the pod, use: kubectl logs <pod-name> -n kube-system
								               Example: kubectl logs calico-node-abcde -n kube-system
					What to Look For in Logs
					              >> Errors like failed to setup network, IPAM error, route not found
					              >> Warnings about IP pool exhaustion or misconfiguration
					              >> Messages about failed pod network setup


	Use tools like nslookup or dig to verify DNS resolution.
			  >>  If nslookup or dig fails, it means CoreDNS might be down or misconfigured.
			  >>  If it works, DNS is likely functioning correctly.
			Run a Temporary Pod with DNS Tools:
				    >> If you don’t already have a pod with nslookup or dig, you can create one like this:
				         |-  	kubectl run dnsutils --image=busybox:1.28 --restart=Never -it --rm --command -- sh
				    >> Use nslookup to Test DNS Resolution
					         syntax:- nslookup <service-name>.<namespace>.svc.cluster.local
				         	 Example:- nslookup my-service.default.svc.cluster.local
				    >> You can also test external domains:-  nslookup google.com

  >> Use dig (if available):
		 syntax:- dig <service-name>.<namespace>.svc.cluster.local
		  Example:- dig my-service.default.svc.cluster.local		

 When pods are on different nodes and can’t communicate, it could be due to node-level routing or firewall rules.
		        >>  Each Kubernetes node has its own network interface. When a pod on Node A tries to talk to a pod on Node B, the traffic must go 
                            through the underlying network infrastructure — like your cloud provider’s VPC or your on-prem network.
			
			How to Check Node-Level Routing and Firewall Rules:
			       >> Check Node IPs: kubectl get nodes -o wide  ---> This shows the internal IPs of each node.
			       >> Ping Between Nodes: SSH into one node and try to ping another node’s internal IP:- ping <node-2-ip>
			       >> If this fails, it’s likely a routing or firewall issue.

Check Cloud Firewall Rules (Security Groups):
   >>	If you're on a cloud provider like AWS, GCP, or Azure:
			       |- Make sure inbound and outbound rules allow traffic on:
			       |- All ports between nodes (for pod traffic)
			       |- ICMP (for ping, optional)
			       |- TCP/UDP 10250, 30000–32767 (for kubelet and NodePort)


For example, in one project, pods on different nodes couldn’t communicate. After investigation, I found that the Calico CNI plugin was misconfigured
and the IP pool didn’t match the node subnet. Fixing the IP pool resolved the issue.”


=========================================================================================


 Pod Eviction Due to Resource Pressure:
========================================
When pods are being evicted due to resource pressure, it means the Kubernetes node is running low on resources like CPU, memory, or disk.
This causes the kubelet to evict pods to protect the node and keep the system stable.
	 Symptoms:-
		|- Pods are terminated unexpectedly.
		|- You see events like Pod was evicted or Evicted due to memory pressure.
		|- Services become intermittently unavailable.

How to Confirm It:
	|- Check Pod Events
		kubectl describe pod <pod-name> -n <namespace>
		Look for:
		--------
		  Status: Failed
		  Reason: Evicted
		  Message: The node was low on resource: memory.

	|- Check Node Conditions
		kubectl describe node <node-name>
		Look for:
		--------
		  MemoryPressure: True
		  DiskPressure: True

	|- Check Resource Usage
		kubectl top nodes
		kubectl top pods -n <namespace>

Fixing Pod Eviction Due to Resource Pressure:
---------------------------------------------
	Step 1: Identify the Evicted Pods:
			|- kubectl get pods --all-namespaces --field-selector=status.phase=Failed
			|- Then describe one of the evicted pods:
				|-kubectl describe pod <pod-name> -n <namespace>
				|-Look for messages like:
					|-The node was low on resource: memory. Container was using X, but limit is Y.
	Step 2: Check Node Resource Usage:
			|- kubectl top nodes
				This shows CPU and memory usage per node. Look for nodes that are close to 100% usage
	Step 3: Check Pod Resource Usage:
			|- kubectl top pods -n <namespace>
				Identify pods using excessive memory or CPU.
	Step 4: Set Resource Requests and Limits
			|- Edit your pod or deployment YAML to include:
					
				resources:
 				  requests:
   				     memory: "256Mi"
                                     cpu: "250m"
                                  limits:
                                    memory: "512Mi"
                                    cpu: "500m"
			|- Apply the changes: kubectl apply -f <your-deployment>.yaml
	
	 Step 5: Scale the Cluster:
			|- If nodes are consistently under pressure, add more nodes:
				>> For managed clusters (EKS, GKE, AKS), increase node group size.
				>> For self-managed clusters, add more worker nodes.
	
	 Step 6: Clean Up Unused Resources
			|- Delete unused pods, jobs, or deployments.
			|- Remove old logs or temporary files if disk pressure is the issue.

	 Step 7: Monitor Continuously:
			|- Use tools like:
				>> Prometheus + Grafana for dashboards
				>> metrics-server for kubectl top
				>> Kube-state-metrics for alerts

===============================================================================================================================================
