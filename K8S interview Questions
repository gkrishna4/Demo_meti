1)  CrashLoopBackOff
=================
 The container in the pod is repeatedly crashing and restarting. Kubernetes attempts to start the container, it crashes, and Kubernetes tries to start it again in a loop.

Resolution:
==========
Check Logs: Use kubectl logs <pod-name> -c <container-name> to see the output of the container before it crashes.
Describe Pod: Use kubectl describe pod <pod-name> to get detailed information about the pod’s state and any events associated with it.
Fix Application Errors: Identify and fix the application code or configuration issues causing the crash.
Check Resource Limits: Ensure the pod has enough CPU and memory resources allocated. Adjust resource requests and limits in the pod specification if necessary.

2)ImagePullBackOff / ErrImagePull:  Kubernetes is unable to pull the container image from the registry.
==================================
Check Image Name: Verify that the image name and tag are correct.
Check Registry Credentials: If using a private registry, ensure your Kubernetes cluster has the correct credentials. This can be configured using secrets.

kubectl create secret docker-registry <secret-name> --docker-server=<registry-server> --docker-username=<username> --docker-password=<password> --docker-email=<email>

Check Network Connectivity: Ensure that the nodes have network access to the image registry.

3) Pending: The pod is not being scheduled onto a node.
=========================================================
Check Node Resources: Use kubectl describe pod <pod-name> to see if there are resource constraints preventing scheduling.
Review Resource Requests and Limits: Ensure the pod’s resource requests and limits are reasonable.
Node Availability: Ensure there are sufficient nodes in the cluster and they are not tainted in a way that prevents scheduling.
Use kubectl get nodes to check the status of nodes

4) ContainerCreating:The container is stuck in the creating state.
===================================================================
Check Node Status: Ensure the node where the pod is scheduled is healthy.
kubectl get nodes
Check Container Runtime Logs: Look at the container runtime logs (Docker or containerd) on the node for more information.
Check Volume Attachments: If using persistent volumes, ensure they are correctly attached and available.

5)CreateContainerConfigError:There’s an issue with the container configuration.
================================================================================
Check Configuration: Ensure all environment variables, secrets, and config maps referenced in the pod spec are correctly specified and available.
Describe Pod: Use kubectl describe pod <pod-name> to get more details about the error.

6) OOMKilled: The container was killed because it ran out of memory.
========================================================================
Increase Memory Limits: Adjust the memory limits in the pod specification
resources:
  requests:
    memory: "512Mi"
  limits:
    memory: "1Gi"
Optimize Application Memory Usage: Investigate and optimize the application code to reduce memory usage.

7) NodeNotReady:The node is not in a ready state to schedule pods.
======================================================================
Check Node Health: Use kubectl describe node <node-name> to check for issues.
Check Node Resources: Ensure the node has sufficient CPU and memory and is not under heavy load.
Restart Node Services: Restart kubelet and other node services if needed.

8)  Unauthorized: You don’t have permission to perform an action.
=================================================================
Check RBAC Policies: Ensure your user or service account has the necessary permissions.
kubectl get roles --all-namespaces
kubectl get rolebindings --all-namespaces
Update Role Bindings: Adjust role and role bindings to grant the required permissions.

9)Service Unreachable:Unable to reach a service from within the cluster.
========================================================================
Check Service and Endpoints: Use kubectl get svc and kubectl get endpoints to ensure the service and endpoints are correctly configured.
kubectl get svc
kubectl get endpoints
Check Network Policies: Ensure network policies are not blocking the traffic.
DNS Issues: Verify that DNS is correctly resolving service names. Check the CoreDNS logs for issues.

10)Port Already in Use:The specified port is already in use on the node.
==========================================================================
Check for Conflicts: Ensure no other service on the node is using the same port.
Change Port: Update the service or pod to use a different port.


11) PersistentVolume (PV) Not Bound: PersistentVolumeClaims (PVCs) are stuck in the "Pending" state because they are not bound to a PersistentVolume (PV).
==========================================================================================================================================================
Check PVC and PV Definitions: Ensure that the PVC request matches an available PV in terms of storage class, size, and access modes.
kubectl get pvc
kubectl get pv
Describe PVC: Use kubectl describe pvc <pvc-name> to get more details about why the binding is failing
kubectl describe pvc <pvc-name>
Adjust PV/PVC Specs: Modify the PV and PVC specifications to ensure they are compatible.

12)  PVC Not Created:PVC is not created or is stuck in the "Pending" state.
============================================================================
Check Storage Class: Ensure the specified storage class exists and is correctly configured.
kubectl get storageclass
Describe PVC: Use kubectl describe pvc <pvc-name> to get details on why it might not be getting created.
Check Cluster Provisioner: Ensure that the storage provisioner (such as AWS EBS, GCE PD, etc.) is properly set up and functional.

13) A pod reports a "Read-only file system" error when trying to write to a volume.
=========================================================================================
Check Access Modes: Ensure the PVC is created with the correct access mode (ReadWriteOnce, ReadOnlyMany, ReadWriteMany).
accessModes:
  - ReadWriteOnce

Check Node Status: Ensure the node hosting the pod has proper access to the underlying storage.
Check Volume Permissions: Verify that the volume's file system permissions allow write access to the container's user.


14) The pod is stuck in the "ContainerCreating" state due to volume mount timeout.
====================================================================================
Check Node and Volume Connectivity: Ensure the node can access the volume's backend storage system.
Describe Pod: Use kubectl describe pod <pod-name> to get details on the mount failure.
Check Storage Provider Logs: Look at the logs of the storage provider to diagnose connectivity or permission issues.


15) Data stored in a volume is lost or corrupted.
==================================================
Use Backup and Restore: Regularly back up your data and have a restore strategy in place.
Check Storage System Health: Ensure the underlying storage system is healthy and not experiencing issues.
Check Application Logs: Sometimes data corruption can be due to application-level issues, so check the application logs for errors.


16) A volume is not detaching from a node after a pod is terminated.
=====================================================================
Check Node Logs: Look at the node's kubelet logs for errors related to volume detachment.
Manual Detach: Depending on your storage provider, you might need to manually detach the volume using cloud provider tools (e.g., AWS CLI for EBS volumes).
Ensure Proper Pod Termination: Make sure the pod is properly terminated and not stuck in a terminating state.

17) A volume is not attaching to the node where the pod is scheduled.
======================================================================
Check Node and Volume Availability: Ensure the node is available and the volume is not already attached elsewhere.
Describe Pod: Use kubectl describe pod <pod-name> to see events related to volume attachment failures.
Check Cloud Provider Limits: Some cloud providers have limits on the number of volumes that can be attached to a single node. Ensure you are within these limits.


18) Issues related to the storage class configuration.
======================================================
Check Storage Class Definition: Ensure the storage class is correctly defined and available
kubectl get storageclass
Check Provisioner: Make sure the provisioner specified in the storage class is installed and functional.
Describe PVC: Use kubectl describe pvc <pvc-name> to see if there are issues related to the storage class.

19) The Ingress resource is not routing traffic as expected.
==============================================================
Check Ingress Resource Configuration: Ensure the Ingress resource is correctly defined, including the correct hostname, paths, and backend services.
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: example-ingress
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
spec:
  rules:
    - host: example.com
      http:
        paths:
          - path: /
            pathType: Prefix
            backend:
              service:
                name: example-service
                port:
                  number: 80
Check Ingress Controller Logs: Use kubectl logs <ingress-controller-pod> to check for errors or warnings.
kubectl logs <ingress-controller-pod>

Describe Ingress: Use kubectl describe ingress <ingress-name> to get detailed information and see if there are any events or errors.
kubectl describe ingress <ingress-name>


20) The Ingress resource returns a 404 error, indicating that the requested resource could not be found.
==========================================================================================================
Check Path and Host Configuration: Ensure the path and host in the Ingress resource match the request.
Check Service Configuration: Verify that the backend service is correctly defined and exposed.
Check DNS Configuration: Ensure the DNS records point to the correct IP address of the Load Balancer.


21) Problems with SSL/TLS certificates, such as not being served or invalid certificates.
===========================================================================================
Check TLS Configuration: Ensure the TLS section in the Ingress resource is correctly configured.
spec:
  tls:
    - hosts:
        - example.com
      secretName: example-tls

Check Secret: Verify that the secret containing the TLS certificate exists and is correctly configured.

kubectl get secret example-tls

Check Certificate Validity: Ensure the TLS certificate is valid and not expired.

22) The Ingress Controller is unable to reach the backend service.
===================================================================
Check Service and Endpoints: Ensure the service and its endpoints are correctly configured and running.
kubectl get svc
kubectl get endpoints

Check Network Policies: Ensure there are no network policies blocking traffic to the backend service.
Check Pod Readiness: Ensure the pods behind the service are ready and able to handle traffic.


23)The Ingress Controller itself is not functioning properly.
============================================================
Check Ingress Controller Deployment: Ensure the Ingress Controller pods are running and healthy.
kubectl get pods -n ingress-nginx

Check Logs: Review the Ingress Controller logs for any errors or warnings.
kubectl logs <ingress-controller-pod> -n ingress-nginx

Check RBAC Permissions: Ensure the Ingress Controller has the necessary permissions to read Ingress resources and manage service


24) DNS does not resolve to the correct IP address.
=====================================================
Check DNS Records: Ensure the DNS records are correctly configured to point to the Load Balancer's IP address.
Check External-DNS Configuration: If using External-DNS, ensure it is properly configured and running.


25)  The Load Balancer is not provisioned or is stuck in the "Pending" state.
==============================================================================
Check Service Type: Ensure the service type is LoadBalancer
apiVersion: v1
kind: Service
metadata:
  name: example-service
spec:
  type: LoadBalancer
  ports:
    - port: 80
  selector:
    app: example

Check Cloud Provider Integration: Ensure your Kubernetes cluster is correctly integrated with your cloud provider to provision Load Balancers.
Check Node Permissions: Ensure the nodes have the necessary permissions to create Load Balancers in the cloud provider


26) The Load Balancer health checks are failing, causing traffic to not be routed.
====================================================================================
Check Health Check Configuration: Ensure the health checks are correctly configured for the backend service.
Check Pod Health: Ensure the pods behind the service are healthy and passing their readiness and liveness probes.
===================================================================================




General Troubleshooting Commands:
=================================
          View Logs: kubectl logs <pod-name> [-c <container-name>]
          Describe Resource: kubectl describe pod <pod-name> or kubectl describe svc <service-name>
          Check Events: kubectl get events
          Check Node Status: kubectl get nodes
          List Pods: kubectl get pods --all-namespaces
          List Services: kubectl get svc

Get PVCs: kubectl get pvc
Get PVs: kubectl get pv
Describe PVC: kubectl describe pvc <pvc-name>
Describe PV: kubectl describe pv <pv-name>
Get Storage Classes: kubectl get storageclass
Describe Pod: kubectl describe pod <pod-name>
View Logs: kubectl logs <pod-name>

View Ingress Resources: kubectl get ingress
Describe Ingress: kubectl describe ingress <ingress-name>
Check Logs: kubectl logs <ingress-controller-pod>
Check Services: kubectl get svc
Check Endpoints: kubectl get endpoints
Check Nodes: kubectl get nodes
Check Pods: kubectl get pods --all-namespaces
